# Lab8 : Locks
### 开始日期：2023.02.19
### 完成日期：2023.02.24
- ### `心得`
  
- ### `内存分配器优化`
  - 实验分析
    - xv6原有的内存分配方案是通过链表维护一个空闲列表，CPU所有的核心都共享这个空闲列表。当进程需要新内存的时候，会检查链表头部的页是否为空，如果不为空就将其作为自己的新空间，然后让链表头指向下一个节点。
    - 在分配新内存的过程中，会涉及多步链表的操作，如果在此期间发生中断，另一个核心同时对链表进行操纵，就会破坏空闲列表的一致性。为了避免这钟情况，就需要在对链表进行操作之前加锁，直到操作完成再释放锁。
    - 然而，使用锁会导致内存分配失去并行性，遇到频繁分配或释放内存的情况时，就会花费大量的时间在acquire上。该实验的目的是重新设计内存分配器，消除锁争用导致的时间浪费。
  - 实验思路
    - 为了提高并行度，首先想到的是提高锁的细粒度。原先的分配策略是所有CPU共用一个空闲列表和一把锁，这就导致同一时刻只有一个CPU能分配内存，为了解决这个问题，我们需要让每个CPU维护一个空闲列表，每个列表都有自己的锁。
    - kmem结构体记录了空闲列表和锁，我们为每个CPU都创建一个kmem结构体，并且把所有结构体放在数组中，然后在kinit中初始化每个CPU的锁。
    - 在kfree中，我们使用cpuid()获取当前的CPU编号，通过编号进行索引，获取当前CPU的空闲列表和锁，然后对列表进行操作。此外，为了防止中断带来的影响，需要加上push_off()和pop_off()。
    - 比较麻烦的是kalloc，由于每个CPU有各自的空闲列表，当某个CPU自己的空闲列表为空时，其他CPU可能还有空闲内存。此时，我们需要让该CPU窃取其他CPU的空闲内存。我们使用for循环遍历数组去查找是否空闲内存，找到空闲空间后加上对应CPU的锁将该空间分配给当前CPU即可。
- ### `Buffer cache并行优化`
  - 实验分析
    - xv6原先的buffer cache系统是通过一个大的循环双链表实现的，所有的CPU共享同一个链表，因此所有涉及到链表的操作都需要加锁，如果操作比较久（比如遍历整个链表寻找空闲的buffer），就会导致锁争夺，从而浪费大量的时间。
    - 要解决这个问题可以使用哈希桶，将blockno映射到对应的桶内，寻找blockno的buffer的时候先对它的桶加锁，去桶内寻找一次，没找到就尝试复用桶内空闲的buffer。
    - 如果仍未找到，则需要遍历其他的桶，从其他的桶内复用buffer。
  - 实验思路
    - 首先创建hashbuf结构体存储桶锁和头节点，基于hashbuf创建bcache，bcache中保存hashbuf数组，桶的个数设置为奇数13从而最大限度减少冲突。
    - binit()中初始化每个桶的头节点和锁，头节点的next和pre都要指向自己，表示链表为空。
    - bget()中为blockno分配buffer，具体会涉及到3个步骤
      - 在当前桶内寻找blockno
      - 在当前桶内复用blockno
      - 在其他桶内复用blockno，这里先需要获取其他桶的锁，防止出现一致性问题。为了防止死锁最好先使用holding进行判断，如果其他桶的锁已经被占用了就直接放弃获取。
    - brelse()把buf释放回对应的桶内，释放的过程中要有对应桶的锁。
    - bpin()和bunpin()也要修改为对桶加锁。
    - LRU算法
      - 为了寻找最长时间未使用的buffer，需要使用时间戳对buffer进行标记，在释放buffer的时候更新时间戳，bget()中基于时间戳选择最合适的buffer。
- ### `完成！`
![Image text](https://raw.githubusercontent.com/JennyTurtles/MIT6.S081-2020-labs/lock/user/lab8%20完成.png)
